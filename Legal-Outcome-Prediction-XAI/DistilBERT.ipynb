{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAW_-IkI2gcp"
      },
      "source": [
        "## **Reading the Dataset as a DataFrame**"
      ],
      "id": "rAW_-IkI2gcp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcjBRFTMJ_E1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "BcjBRFTMJ_E1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ed8f8c6"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/Final Year Paper Work/Complete Summarized Dataset.xlsx', header=0,index_col=0)\n",
        "\n",
        "df = df.dropna() # To remove any None values\n",
        "df.head()"
      ],
      "id": "1ed8f8c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7KHZNmG25k_"
      },
      "source": [
        "##**Splitting the data for complete, extractive and abstractive summarized texts**"
      ],
      "id": "I7KHZNmG25k_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxRlD5af11l6"
      },
      "outputs": [],
      "source": [
        "# Complete Data\n",
        "com_sentences = df[['Judgement','Judgement Status']]\n",
        "\n",
        "# Abstractive Data\n",
        "abs_sentences = df[['Abstractive Summarized Judgements','Judgement Status']]\n",
        "\n",
        "# Extractive Data\n",
        "ext_sentences = df[['Extractive Summarized Judgements','Judgement Status']]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train and Test Split for Complete Data\n",
        "df_train_com, df_test_com = train_test_split(com_sentences, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train and Test Split for Abstractive Data\n",
        "df_train_abs, df_test_abs = train_test_split(abs_sentences, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train and Test Split for Extractive Data\n",
        "df_train_ext, df_test_ext = train_test_split(ext_sentences, test_size=0.25, random_state=42)"
      ],
      "id": "GxRlD5af11l6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF6_a-mB32An"
      },
      "source": [
        "## **Converting the Judgement Status to Categorical Values**"
      ],
      "id": "SF6_a-mB32An"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MVXTrx03Vhc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Complete Data\n",
        "y_train_com = to_categorical(df_train_com['Judgement Status'])\n",
        "y_test_com = to_categorical(df_test_com['Judgement Status'])\n",
        "\n",
        "# Abstractive Data\n",
        "y_train_abs = to_categorical(df_train_abs['Judgement Status'])\n",
        "y_test_abs = to_categorical(df_test_abs['Judgement Status'])\n",
        "\n",
        "# Extractive Data\n",
        "y_train_ext = to_categorical(df_train_ext['Judgement Status'])\n",
        "y_test_ext = to_categorical(df_test_ext['Judgement Status'])"
      ],
      "id": "2MVXTrx03Vhc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Statements**"
      ],
      "metadata": {
        "id": "0lGGcgLAdKbx"
      },
      "id": "0lGGcgLAdKbx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPP8zyUJFRGh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ],
      "id": "WPP8zyUJFRGh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dtxf7uEvFRGh"
      },
      "outputs": [],
      "source": [
        "# Import Statements\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,Dropout, Input\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import regularizers\n",
        "from transformers import BertConfig"
      ],
      "id": "Dtxf7uEvFRGh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPQGqBg9FRGh"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "\n",
        "dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ],
      "id": "qPQGqBg9FRGh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnzUZFu_FRGi"
      },
      "source": [
        "##**1) Complete Data**"
      ],
      "id": "AnzUZFu_FRGi"
    },
    {
      "cell_type": "code",
      "source": [
        "df_com_dbert = df\n",
        "df_com_dbert = df_com_dbert.rename(columns = {'Judgement Status': 'label', 'Judgement': 'text'}, inplace = False)\n",
        "print('Available labels: ',df_com_dbert.label.unique())\n",
        "\n",
        "num_classes_com_dbert = len(df_com_dbert.label.unique())\n",
        "df_com_dbert.head()"
      ],
      "metadata": {
        "id": "_oJSRu2WGDio"
      },
      "id": "_oJSRu2WGDio",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 100\n",
        "sentences_com_dbert = df_com_dbert['text']\n",
        "labels_com_dbert = df_com_dbert['label']\n",
        "len(sentences_com_dbert), len(labels_com_dbert)"
      ],
      "metadata": {
        "id": "-1fnqwHUGJb-"
      },
      "id": "-1fnqwHUGJb-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Checking if the Tokenizer is working well with out data by giving one sentence as input**"
      ],
      "metadata": {
        "id": "gwUH9KRVKStz"
      },
      "id": "gwUH9KRVKStz"
    },
    {
      "cell_type": "code",
      "source": [
        "dbert_tokenizer.tokenize(sentences_com_dbert[1])\n",
        "\n",
        "dbert_inp_com = dbert_tokenizer.encode_plus(sentences_com_dbert[1],add_special_tokens = True,max_length =20,pad_to_max_length = True,truncation=True)\n",
        "dbert_inp_com"
      ],
      "metadata": {
        "id": "3otVVs3vGOog"
      },
      "id": "3otVVs3vGOog",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_inp_com_dbert = np.asarray(dbert_inp_com['input_ids'])\n",
        "mask_inp_com_dbert = np.asarray(dbert_inp_com['attention_mask'])\n",
        "out_com_dbert = dbert_model([id_inp_com_dbert.reshape(1,-1),mask_inp_com_dbert.reshape(1,-1)])\n",
        "type(out_com_dbert),out_com_dbert"
      ],
      "metadata": {
        "id": "dc9iImtQGmtV"
      },
      "id": "dc9iImtQGmtV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbert_tokenizer.decode(dbert_inp_com['input_ids'])"
      ],
      "metadata": {
        "id": "zteJQ4jvG5cE"
      },
      "id": "zteJQ4jvG5cE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Since tokenizer is working well, run the tokenizer with all sentences**"
      ],
      "metadata": {
        "id": "lzMYKBISKY5L"
      },
      "id": "lzMYKBISKY5L"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    inps = Input(shape = (max_len,), dtype='int64')\n",
        "    masks= Input(shape = (max_len,), dtype='int64')\n",
        "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    dense = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
        "    dropout= Dropout(0.5)(dense)\n",
        "    pred = Dense(4, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "model_com_dbert = create_model()"
      ],
      "metadata": {
        "id": "qNM5QJ9-G9_r"
      },
      "id": "qNM5QJ9-G9_r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_com_dbert = []\n",
        "attention_masks_com_dbert = []\n",
        "\n",
        "for sent in sentences_com_dbert:\n",
        "    dbert_inps_com = dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids_com_dbert.append(dbert_inps_com['input_ids'])\n",
        "    attention_masks_com_dbert.append(dbert_inps_com['attention_mask'])\n",
        "\n",
        "input_ids_com_dbert = np.asarray(input_ids_com_dbert)\n",
        "attention_masks_com_dbert = np.array(attention_masks_com_dbert)\n",
        "labels_com_dbert = np.array(labels_com_dbert)\n",
        "len(input_ids_com_dbert),len(attention_masks_com_dbert),len(labels_com_dbert)"
      ],
      "metadata": {
        "id": "QBQxpN46HK5N"
      },
      "id": "QBQxpN46HK5N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inp_com_dbert, val_inp_com_dbert, train_label_com_dbert, val_label_com_dbert, train_mask_com_dbert, val_mask_com_dbert = train_test_split(input_ids_com_dbert,labels_com_dbert,attention_masks_com_dbert,test_size=0.25)\n",
        "\n",
        "print('Train inp shape {} Val input shape {}\\nTrain label shape {} Val label shape {}\\nTrain attention mask shape {} Val attention mask shape {}'.format(train_inp_com_dbert.shape,val_inp_com_dbert.shape,train_label_com_dbert.shape,val_label_com_dbert.shape,train_mask_com_dbert.shape,val_mask_com_dbert.shape))\n",
        "\n",
        "loss_com_dbert = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric_com_dbert = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer_com_dbert = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "model_com_dbert.compile(loss=loss_com_dbert, optimizer=optimizer_com_dbert, metrics=[metric_com_dbert])"
      ],
      "metadata": {
        "id": "iMnFiuYEHji7"
      },
      "id": "iMnFiuYEHji7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_com_dbert = model_com_dbert.fit([train_inp_com_dbert,train_mask_com_dbert],train_label_com_dbert,batch_size = 32,epochs = 10,validation_data=([val_inp_com_dbert,val_mask_com_dbert],val_label_com_dbert))"
      ],
      "metadata": {
        "id": "OfmxwCy9Kr1a"
      },
      "id": "OfmxwCy9Kr1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_com_dbert = model_com_dbert.predict([val_inp_com_dbert,val_mask_com_dbert],batch_size=16)"
      ],
      "metadata": {
        "id": "5dQLTVqTK_V1"
      },
      "id": "5dQLTVqTK_V1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels_com_dbert = preds_com_dbert.argmax(axis=1)\n",
        "f1_com_dbert = f1_score(val_label_com_dbert,pred_labels_com_dbert,average='weighted')\n",
        "print(\"F1 Score: \",f1_com_dbert)"
      ],
      "metadata": {
        "id": "JiHLOztFLKDx"
      },
      "id": "JiHLOztFLKDx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "\n",
        "plot_graphs(history_com_dbert, \"accuracy\")\n",
        "plot_graphs(history_com_dbert, \"loss\")"
      ],
      "metadata": {
        "id": "J4bwyxKQLVmI"
      },
      "id": "J4bwyxKQLVmI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO0c-SRxFRGl"
      },
      "outputs": [],
      "source": [
        "model_com_dbert.save(\"/content/drive/MyDrive/Final Year Paper Work/H5 Files/COM-DISTILBERT.h5\")"
      ],
      "id": "mO0c-SRxFRGl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZvUJlHNFRGl"
      },
      "source": [
        "##**2) Abstractive Summarized Data**\n"
      ],
      "id": "GZvUJlHNFRGl"
    },
    {
      "cell_type": "code",
      "source": [
        "df_abs_dbert = df\n",
        "df_abs_dbert = df_abs_dbert.rename(columns = {'Judgement Status': 'label', 'Abstractive Summarized Judgements': 'text'}, inplace = False)\n",
        "print('Available labels: ',df_abs_dbert.label.unique())\n",
        "\n",
        "num_classes_abs__dbert = len(df_abs_dbert.label.unique())\n",
        "df_abs_dbert.head()"
      ],
      "metadata": {
        "id": "a2mveSUMMBX9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "a2mveSUMMBX9"
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 100\n",
        "sentences_abs_dbert = df_abs_dbert['text']\n",
        "labels_abs_dbert = df_abs_dbert['label']\n",
        "len(sentences_abs_dbert), len(labels_abs_dbert)"
      ],
      "metadata": {
        "id": "3dUtW3XkMBX-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3dUtW3XkMBX-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Checking if the Tokenizer is working well with out data by giving one sentence as input**"
      ],
      "metadata": {
        "id": "viGEreVSMBX_"
      },
      "id": "viGEreVSMBX_"
    },
    {
      "cell_type": "code",
      "source": [
        "dbert_tokenizer.tokenize(sentences_abs_dbert[1])\n",
        "\n",
        "dbert_inp_abs = dbert_tokenizer.encode_plus(sentences_abs_dbert[1],add_special_tokens = True,max_length =20,pad_to_max_length = True,truncation=True)\n",
        "dbert_inp_abs"
      ],
      "metadata": {
        "id": "PwGqmd2iMBX_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "PwGqmd2iMBX_"
    },
    {
      "cell_type": "code",
      "source": [
        "id_inp_abs_dbert = np.asarray(dbert_inp_abs['input_ids'])\n",
        "mask_inp_abs_dbert = np.asarray(dbert_inp_abs['attention_mask'])\n",
        "out_abs_dbert = dbert_model([id_inp_abs_dbert.reshape(1,-1),mask_inp_abs_dbert.reshape(1,-1)])\n",
        "type(out_abs_dbert),out_abs_dbert"
      ],
      "metadata": {
        "id": "nfrteDtUMBYA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nfrteDtUMBYA"
    },
    {
      "cell_type": "code",
      "source": [
        "dbert_tokenizer.decode(dbert_inp_abs['input_ids'])"
      ],
      "metadata": {
        "id": "-cOtiHGFMBYA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-cOtiHGFMBYA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Since tokenizer is working well, run the tokenizer with all sentences**"
      ],
      "metadata": {
        "id": "OtjPpu6UMBYA"
      },
      "id": "OtjPpu6UMBYA"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    inps = Input(shape = (max_len,), dtype='int64')\n",
        "    masks= Input(shape = (max_len,), dtype='int64')\n",
        "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    dense = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
        "    dropout= Dropout(0.5)(dense)\n",
        "    pred = Dense(4, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "model_abs_dbert = create_model()"
      ],
      "metadata": {
        "id": "y4fn3WkCMBYA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "y4fn3WkCMBYA"
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_abs_dbert = []\n",
        "attention_masks_abs_dbert = []\n",
        "\n",
        "for sent in sentences_abs_dbert:\n",
        "    dbert_inps_abs = dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids_abs_dbert.append(dbert_inps_abs['input_ids'])\n",
        "    attention_masks_abs_dbert.append(dbert_inps_abs['attention_mask'])\n",
        "\n",
        "input_ids_abs_dbert = np.asarray(input_ids_abs_dbert)\n",
        "attention_masks_abs_dbert = np.array(attention_masks_abs_dbert)\n",
        "labels_abs_dbert = np.array(labels_abs_dbert)\n",
        "len(input_ids_abs_dbert),len(attention_masks_abs_dbert),len(labels_abs_dbert)"
      ],
      "metadata": {
        "id": "cfeJD4adMBYA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cfeJD4adMBYA"
    },
    {
      "cell_type": "code",
      "source": [
        "train_inp_abs_dbert, val_inp_abs_dbert, train_label_abs_dbert, val_label_abs_dbert, train_mask_abs_dbert, val_mask_abs_dbert = train_test_split(input_ids_abs_dbert,labels_abs_dbert,attention_masks_abs_dbert,test_size=0.25)\n",
        "\n",
        "print('Train inp shape {} Val input shape {}\\nTrain label shape {} Val label shape {}\\nTrain attention mask shape {} Val attention mask shape {}'.format(train_inp_abs_dbert.shape,val_inp_abs_dbert.shape,train_label_abs_dbert.shape,val_label_abs_dbert.shape,train_mask_abs_dbert.shape,val_mask_abs_dbert.shape))\n",
        "\n",
        "loss_abs_dbert = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric_abs_dbert = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer_abs_dbert = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "model_abs_dbert.compile(loss=loss_abs_dbert, optimizer=optimizer_abs_dbert, metrics=[metric_abs_dbert])"
      ],
      "metadata": {
        "id": "DENt-AouMBYA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DENt-AouMBYA"
    },
    {
      "cell_type": "code",
      "source": [
        "history_abs_dbert = model_abs_dbert.fit([train_inp_abs_dbert,train_mask_abs_dbert],train_label_abs_dbert,batch_size = 32,epochs = 10,validation_data=([val_inp_abs_dbert,val_mask_abs_dbert],val_label_abs_dbert))"
      ],
      "metadata": {
        "id": "vCxGmgEBMBYA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vCxGmgEBMBYA"
    },
    {
      "cell_type": "code",
      "source": [
        "preds_abs_dbert = model_abs_dbert.predict([val_inp_abs_dbert,val_mask_abs_dbert],batch_size=16)"
      ],
      "metadata": {
        "id": "VuWmWQAmMBYB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "VuWmWQAmMBYB"
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels_abs_dbert = preds_abs_dbert.argmax(axis=1)\n",
        "f1_abs_dbert = f1_score(val_label_abs_dbert,pred_labels_abs_dbert,average='weighted')\n",
        "print(\"F1 Score: \",f1_abs_dbert)"
      ],
      "metadata": {
        "id": "j67k5gXqMBYB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "j67k5gXqMBYB"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "\n",
        "plot_graphs(history_abs_dbert, \"accuracy\")\n",
        "plot_graphs(history_abs_dbert, \"loss\")"
      ],
      "metadata": {
        "id": "jZjoxQwXMBYB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "jZjoxQwXMBYB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAJqWKEpMBYB"
      },
      "outputs": [],
      "source": [
        "model_abs_dbert.save(\"/content/drive/MyDrive/Final Year Paper Work/H5 Files/ABS-DISTILBERT.h5\")"
      ],
      "id": "JAJqWKEpMBYB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1eubOqhFRGu"
      },
      "source": [
        "## **3) Extractive Summarized Data**"
      ],
      "id": "X1eubOqhFRGu"
    },
    {
      "cell_type": "code",
      "source": [
        "df_ext_dbert = df\n",
        "df_ext_dbert = df_ext_dbert.rename(columns = {'Judgement Status': 'label', 'Extractive Summarized Judgements': 'text'}, inplace = False)\n",
        "print('Available labels: ',df_ext_dbert.label.unique())\n",
        "\n",
        "num_classes_ext_dbert = len(df_ext_dbert.label.unique())\n",
        "df_ext_dbert.head()"
      ],
      "metadata": {
        "id": "QxumW7uhM_jQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QxumW7uhM_jQ"
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 100\n",
        "sentences_ext_dbert = df_ext_dbert['text']\n",
        "labels_ext_dbert = df_ext_dbert['label']\n",
        "len(sentences_ext_dbert), len(labels_ext_dbert)"
      ],
      "metadata": {
        "id": "CqADe40-M_jR"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CqADe40-M_jR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Checking if the Tokenizer is working well with out data by giving one sentence as input**"
      ],
      "metadata": {
        "id": "lWxn20gJM_jR"
      },
      "id": "lWxn20gJM_jR"
    },
    {
      "cell_type": "code",
      "source": [
        "dbert_tokenizer.tokenize(sentences_ext_dbert[1])\n",
        "\n",
        "dbert_inp_ext = dbert_tokenizer.encode_plus(sentences_ext_dbert[1],add_special_tokens = True,max_length =20,pad_to_max_length = True,truncation=True)\n",
        "dbert_inp_ext"
      ],
      "metadata": {
        "id": "KZm4izyyM_jS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "KZm4izyyM_jS"
    },
    {
      "cell_type": "code",
      "source": [
        "id_inp_ext_dbert = np.asarray(dbert_inp_ext['input_ids'])\n",
        "mask_inp_ext_dbert = np.asarray(dbert_inp_ext['attention_mask'])\n",
        "out_ext_dbert = dbert_model([id_inp_ext_dbert.reshape(1,-1),mask_inp_ext_dbert.reshape(1,-1)])\n",
        "type(out_ext_dbert),out_ext_dbert"
      ],
      "metadata": {
        "id": "cXK6pCTYM_jS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cXK6pCTYM_jS"
    },
    {
      "cell_type": "code",
      "source": [
        "dbert_tokenizer.decode(dbert_inp_ext['input_ids'])"
      ],
      "metadata": {
        "id": "dPwr0x7xM_jS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dPwr0x7xM_jS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Since tokenizer is working well, run the tokenizer with all sentences**"
      ],
      "metadata": {
        "id": "nycKomJxM_jT"
      },
      "id": "nycKomJxM_jT"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    inps = Input(shape = (max_len,), dtype='int64')\n",
        "    masks= Input(shape = (max_len,), dtype='int64')\n",
        "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    dense = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
        "    dropout= Dropout(0.5)(dense)\n",
        "    pred = Dense(4, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "model_ext_dbert = create_model()"
      ],
      "metadata": {
        "id": "hRKIbRvpM_jT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hRKIbRvpM_jT"
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_ext_dbert = []\n",
        "attention_masks_ext_dbert = []\n",
        "\n",
        "for sent in sentences_ext_dbert:\n",
        "    dbert_inps_ext = dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids_ext_dbert.append(dbert_inps_ext['input_ids'])\n",
        "    attention_masks_ext_dbert.append(dbert_inps_ext['attention_mask'])\n",
        "\n",
        "input_ids_ext_dbert = np.asarray(input_ids_ext_dbert)\n",
        "attention_masks_ext_dbert = np.array(attention_masks_ext_dbert)\n",
        "labels_ext_dbert = np.array(labels_ext_dbert)\n",
        "len(input_ids_ext_dbert),len(attention_masks_ext_dbert),len(labels_ext_dbert)"
      ],
      "metadata": {
        "id": "R6Ngch5FM_jT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "R6Ngch5FM_jT"
    },
    {
      "cell_type": "code",
      "source": [
        "train_inp_ext_dbert, val_inp_ext_dbert, train_label_ext_dbert, val_label_ext_dbert, train_mask_ext_dbert, val_mask_ext_dbert = train_test_split(input_ids_ext_dbert,labels_ext_dbert,attention_masks_ext_dbert,test_size=0.25)\n",
        "\n",
        "print('Train inp shape {} Val input shape {}\\nTrain label shape {} Val label shape {}\\nTrain attention mask shape {} Val attention mask shape {}'.format(train_inp_ext_dbert.shape,val_inp_ext_dbert.shape,train_label_ext_dbert.shape,val_label_ext_dbert.shape,train_mask_ext_dbert.shape,val_mask_ext_dbert.shape))\n",
        "\n",
        "loss_ext_dbert = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric_ext_dbert = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer_ext_dbert = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "model_ext_dbert.compile(loss=loss_ext_dbert, optimizer=optimizer_ext_dbert, metrics=[metric_ext_dbert])"
      ],
      "metadata": {
        "id": "1q9p_VdDM_jT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1q9p_VdDM_jT"
    },
    {
      "cell_type": "code",
      "source": [
        "history_ext_dbert = model_ext_dbert.fit([train_inp_ext_dbert,train_mask_ext_dbert],train_label_ext_dbert,batch_size = 32,epochs = 10,validation_data=([val_inp_ext_dbert,val_mask_ext_dbert],val_label_ext_dbert))"
      ],
      "metadata": {
        "id": "4G4KKALQM_jU"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4G4KKALQM_jU"
    },
    {
      "cell_type": "code",
      "source": [
        "preds_ext_dbert = model_ext_dbert.predict([val_inp_dbert_ext_dbert,val_mask_ext_dbert],batch_size=16)"
      ],
      "metadata": {
        "id": "2sTQRRVEM_jU"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2sTQRRVEM_jU"
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels_ext_dbert = preds_ext_dbert.argmax(axis=1)\n",
        "f1_ext_dbert = f1_score(val_label_ext_dbert,pred_labels_ext_dbert,average='weighted')\n",
        "print(\"F1 Score: \",f1_ext_dbert)"
      ],
      "metadata": {
        "id": "qGgbPrleM_jU"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qGgbPrleM_jU"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "\n",
        "plot_graphs(history_ext_dbert, \"accuracy\")\n",
        "plot_graphs(history_ext_dbert, \"loss\")"
      ],
      "metadata": {
        "id": "sTsrTYh7M_jU"
      },
      "execution_count": null,
      "outputs": [],
      "id": "sTsrTYh7M_jU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcZ3l8gEM_jV"
      },
      "outputs": [],
      "source": [
        "model_ext_dbert.save(\"/content/drive/MyDrive/Final Year Paper Work/H5 Files/EXT-DISTILBERT.h5\")"
      ],
      "id": "QcZ3l8gEM_jV"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rAW_-IkI2gcp",
        "I7KHZNmG25k_",
        "SF6_a-mB32An",
        "0lGGcgLAdKbx",
        "AnzUZFu_FRGi",
        "gwUH9KRVKStz",
        "lzMYKBISKY5L",
        "GZvUJlHNFRGl",
        "viGEreVSMBX_",
        "OtjPpu6UMBYA",
        "X1eubOqhFRGu",
        "lWxn20gJM_jR",
        "nycKomJxM_jT"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}