{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Read the Data and Store it in a DataFrame**"
      ],
      "metadata": {
        "id": "WmTabEFNvTOh"
      },
      "id": "WmTabEFNvTOh"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3A2sgIJru2jr"
      },
      "id": "3A2sgIJru2jr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "df_base = pd.read_excel('/content/drive/MyDrive/Final Year Paper Work/Combined Dataset.xlsx', header=0,index_col=0)\n",
        "\n",
        "df_base = df_base.dropna() # To remove any None values\n",
        "df_base.head()"
      ],
      "metadata": {
        "id": "DYHYavtPu8b_"
      },
      "id": "DYHYavtPu8b_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Complete Data\n",
        "sentences = df_base[['Judgement','Judgement Status']]\n",
        "\n",
        "# Train and Test Split for Abstractive Data\n",
        "df_train, df_test = train_test_split(sentences, test_size=0.25, random_state=42)\n",
        "\n",
        "# Complete Data\n",
        "y_train = to_categorical(df_train['Judgement Status'])\n",
        "y_test = to_categorical(df_test['Judgement Status'])"
      ],
      "metadata": {
        "id": "VMDMSmiC4-w1"
      },
      "id": "VMDMSmiC4-w1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_base = df_base.rename(columns = {'Judgement Status': 'label', 'Judgement': 'text'}, inplace = False)\n",
        "print('Available labels: ',df_base.label.unique())\n",
        "\n",
        "num_classes_base = len(df_base.label.unique())\n",
        "df_base.head()"
      ],
      "metadata": {
        "id": "zF4QBnCXu82w"
      },
      "id": "zF4QBnCXu82w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Statements**"
      ],
      "metadata": {
        "id": "hLtX_wdnvaCj"
      },
      "id": "hLtX_wdnvaCj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfMMYYbN9EAm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers lime"
      ],
      "id": "hfMMYYbN9EAm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O1BRc2S9EAm"
      },
      "outputs": [],
      "source": [
        "# Import Statements\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,Dropout, Input\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import regularizers"
      ],
      "id": "4O1BRc2S9EAm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw2iir6j9EAn"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, RobertaTokenizer, TFRobertaModel\n",
        "\n",
        "# Load the tokenizer and model\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "roberta_model = TFRobertaModel.from_pretrained('roberta-base')"
      ],
      "id": "Pw2iir6j9EAn"
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 100\n",
        "sentences_base = df_base['text']\n",
        "labels_base = df_base['label']\n",
        "len(sentences_base), len(labels_base)"
      ],
      "metadata": {
        "id": "nFSXKZuP9EAr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nFSXKZuP9EAr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Checking if the Tokenizer is working well with out data by giving one sentence as input**"
      ],
      "metadata": {
        "id": "EecMIzFG9EAr"
      },
      "id": "EecMIzFG9EAr"
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_tokenizer.tokenize(sentences_base[1])\n",
        "\n",
        "base_inp_com = roberta_tokenizer.encode_plus(sentences_base[1],add_special_tokens = True,max_length =20,pad_to_max_length = True,truncation=True)\n",
        "base_inp_com"
      ],
      "metadata": {
        "id": "ESNxU-dX9EAr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ESNxU-dX9EAr"
    },
    {
      "cell_type": "code",
      "source": [
        "id_inp_base = np.asarray(base_inp_com['input_ids'])\n",
        "mask_inp_base = np.asarray(base_inp_com['attention_mask'])\n",
        "out_base = roberta_model([id_inp_base.reshape(1,-1),mask_inp_base.reshape(1,-1)])\n",
        "type(out_base),out_base"
      ],
      "metadata": {
        "id": "XrjY_Mdq9EAr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "XrjY_Mdq9EAr"
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_tokenizer.decode(base_inp_com['input_ids'])"
      ],
      "metadata": {
        "id": "8P-7QXfn9EAr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8P-7QXfn9EAr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Since tokenizer is working well, run the tokenizer with all sentences**"
      ],
      "metadata": {
        "id": "bMsN4U6M9EAs"
      },
      "id": "bMsN4U6M9EAs"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    inps = Input(shape = (max_len,), dtype='int64')\n",
        "    masks= Input(shape = (max_len,), dtype='int64')\n",
        "    roberta_layer = roberta_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    dense = Dense(128,activation='relu')(roberta_layer)\n",
        "    dropout= Dropout(0.1)(dense)\n",
        "    dense = Dense(32,activation = 'relu')(dropout)\n",
        "    pred = Dense(4, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dense)\n",
        "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "model_base = create_model()"
      ],
      "metadata": {
        "id": "th-r7BRN9EAs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "th-r7BRN9EAs"
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_base = []\n",
        "attention_masks_base = []\n",
        "\n",
        "for sent in sentences_base:\n",
        "    base_inps_com = roberta_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids_base.append(base_inps_com['input_ids'])\n",
        "    attention_masks_base.append(base_inps_com['attention_mask'])\n",
        "\n",
        "input_ids_base = np.asarray(input_ids_base)\n",
        "attention_masks_base = np.array(attention_masks_base)\n",
        "labels_base = np.array(labels_base)\n",
        "len(input_ids_base),len(attention_masks_base),len(labels_base)"
      ],
      "metadata": {
        "id": "TRz-sMLZ9EAs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "TRz-sMLZ9EAs"
    },
    {
      "cell_type": "code",
      "source": [
        "train_inp_base, val_inp_base, train_label_base, val_label_base, train_mask_base, val_mask_base = train_test_split(input_ids_base,labels_base,attention_masks_base,test_size=0.25)\n",
        "\n",
        "print('Train inp shape {} Val input shape {}\\nTrain label shape {} Val label shape {}\\nTrain attention mask shape {} Val attention mask shape {}'.format(train_inp_base.shape,val_inp_base.shape,train_label_base.shape,val_label_base.shape,train_mask_base.shape,val_mask_base.shape))\n",
        "\n",
        "loss_base = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric_base = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer_base = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "model_base.compile(loss=loss_base, optimizer=optimizer_base, metrics=[metric_base])"
      ],
      "metadata": {
        "id": "ul_QGkqA9EAs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ul_QGkqA9EAs"
    },
    {
      "cell_type": "code",
      "source": [
        "history_base = model_base.fit([train_inp_base,train_mask_base],train_label_base,batch_size = 32,epochs = 10,validation_data=([val_inp_base,val_mask_base],val_label_base))"
      ],
      "metadata": {
        "id": "D9JG7QEo9EAs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "D9JG7QEo9EAs"
    },
    {
      "cell_type": "code",
      "source": [
        "preds_base = model_base.predict([val_inp_base,val_mask_base],batch_size=16)"
      ],
      "metadata": {
        "id": "NiQ14DCZ9EAs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NiQ14DCZ9EAs"
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels_base = preds_base.argmax(axis=1)\n",
        "f1_base = f1_score(val_label_base,pred_labels_base,average='weighted')\n",
        "print(\"F1 Score: \",f1_base)"
      ],
      "metadata": {
        "id": "97rwstg-9EAs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "97rwstg-9EAs"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "\n",
        "plot_graphs(history_base, \"accuracy\")\n",
        "plot_graphs(history_base, \"loss\")"
      ],
      "metadata": {
        "id": "TAQXYKw19EAs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "TAQXYKw19EAs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83QbhR9d9EAs"
      },
      "outputs": [],
      "source": [
        "model_base.save(\"/content/drive/MyDrive/Final Year Paper Work/RoBERTa H5 Files/ASSORTED.h5\")"
      ],
      "id": "83QbhR9d9EAs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from lime import lime_text\n",
        "\n",
        "# Define the function to predict using the RoBERT model with truncation and padding\n",
        "def predict_fn(x):\n",
        "    inputs = roberta_tokenizer(x, return_tensors='tf', truncation=True, padding=True, max_length=100)\n",
        "    return model_base.predict([inputs['input_ids'], inputs['attention_mask']])\n",
        "\n",
        "# Create a LIME explainer\n",
        "explainer = lime_text.LimeTextExplainer()\n",
        "\n",
        "# Choose a sample from the test set for explanation\n",
        "sample_index = 0  # You can change this index as needed\n",
        "text_to_explain = df_test['Judgement'].iloc[sample_index]\n",
        "explanation_class = np.argmax(y_test[sample_index])  # Get the index of the maximum value as the class\n",
        "\n",
        "# Generate LIME explanation\n",
        "exp = explainer.explain_instance(\n",
        "    text_to_explain,\n",
        "    predict_fn,\n",
        "    num_features=10,\n",
        "    num_samples=5000,\n",
        "    labels=list(range(y_test.shape[1])) if len(y_test.shape) > 1 else None\n",
        ")\n",
        "\n",
        "# Print and visualize the explanation\n",
        "print('Explanation for class', explanation_class)\n",
        "print(exp.as_list())\n",
        "\n",
        "# Visualize the explanation\n",
        "exp.show_in_notebook(text=text_to_explain)"
      ],
      "metadata": {
        "id": "Xtrm-Dzb4sbM"
      },
      "id": "Xtrm-Dzb4sbM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_test['Judgement Status'].iloc[sample_index])"
      ],
      "metadata": {
        "id": "ChVuxh2H8eXS"
      },
      "id": "ChVuxh2H8eXS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}