{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install lime"
      ],
      "metadata": {
        "id": "hfiS48Htp2t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import pickle\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string"
      ],
      "metadata": {
        "id": "0CIYaDreZ74d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing:"
      ],
      "metadata": {
        "id": "MBwm9-dPrIHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stopwords, lemmatizer, and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "punctuation = set(string.punctuation)"
      ],
      "metadata": {
        "id": "gqOwtAKpatZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess summarized texts\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  #Lowercasing\n",
        "    words = word_tokenize(text) #Tokenization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word not in punctuation] #Stopwords and punctuation removal, Lemmatization\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Load and preprocess the data\n",
        "df = pd.read_excel('/content/Abstractive_english_lime.xlsx') #Loading summarized judgements dataset\n",
        "df['Processed_abstractive_Summaries'] = df['Abstractive Summarized Judgements'].apply(preprocess_text)\n",
        "df = df[['Processed_abstractive_Summaries', 'Judgement Status']]"
      ],
      "metadata": {
        "id": "z9NazaoCblja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating the Embeddings (in this case InLegalBERT)"
      ],
      "metadata": {
        "id": "W65qBXYbrPc4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ifMfvBxZ3bf"
      },
      "outputs": [],
      "source": [
        "# Initialize InLegalBERT tokenizer and model to generate embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained('law-ai/InLegalBERT')\n",
        "inlegalbert_model = AutoModel.from_pretrained('law-ai/InLegalBERT')\n",
        "\n",
        "# Function to convert text to InLegalBERT embeddings\n",
        "def inlegalbert_embed(text, model, tokenizer, max_len=512):\n",
        "    tokens = tokenizer(text, return_tensors='pt', max_length=max_len, truncation=True, padding='max_length')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# Generate InLegalBERT embeddings\n",
        "df['embeddings'] = df['Processed_abstractive_Summaries'].apply(lambda x: inlegalbert_embed(x, inlegalbert_model, tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame of embeddings\n",
        "embeddings = np.vstack(df['embeddings'].values)\n",
        "embeddings_df = pd.DataFrame(embeddings)\n",
        "embeddings_df['Judgement Status'] = df['Judgement Status'].values\n",
        "\n",
        "X = embeddings_df.drop(columns='Judgement Status')\n",
        "y = embeddings_df['Judgement Status']\n",
        "\n",
        "# Split data into train and test sets (75:25)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "u_cNznNig9QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base Models"
      ],
      "metadata": {
        "id": "4dUy-SNwrauo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_models = {\n",
        "    'RandomForest': RandomForestClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'DecisionTree': DecisionTreeClassifier(),\n",
        "    'XGB': XGBClassifier(),\n",
        "    'LGBM': LGBMClassifier(),\n",
        "    'MLP': MLPClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'GaussianNB': GaussianNB()\n",
        "}"
      ],
      "metadata": {
        "id": "FTlU4yDbiA0j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining hyperparameter grids for each model\n",
        "param_grids = {\n",
        "    'RandomForest': {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'max_features': [\"sqrt\", \"log2\"],\n",
        "        'max_depth': [5, 10, 15, 20],\n",
        "        'min_samples_split': [2, 3, 5, 7, 10],\n",
        "        'min_samples_leaf': [1, 2, 3, 4, 5]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'kernel': ['linear', 'rbf', 'poly'],\n",
        "        'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
        "        'degree': [2, 3, 4],\n",
        "        'coef0': [0.0, 0.1, 0.5, 1.0]\n",
        "    },\n",
        "    'DecisionTree': {\n",
        "        'max_depth': [5, 10, 15, 20],\n",
        "        'min_samples_split': [2, 3, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4, 5]\n",
        "    },\n",
        "    'XGB': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0],\n",
        "        'min_child_weight': [1, 3, 5]\n",
        "    },\n",
        "    'LGBM': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0],\n",
        "        'min_child_samples': [10, 20,30]\n",
        "    },\n",
        "    'MLP': {\n",
        "         'hidden_layer_sizes': [(50, 50), (100, 100), (50,)],\n",
        "         'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "         'max_iter': [200, 300, 400, 500],\n",
        "         'activation': ['logistic', 'tanh', 'relu']\n",
        "    },\n",
        "    'KNN': {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'p': [1, 2]\n",
        "    },\n",
        "    'GaussianNB': {}\n",
        "}"
      ],
      "metadata": {
        "id": "jasI7RQqcLjQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning with GridSearchCV"
      ],
      "metadata": {
        "id": "rYPBr1O4rdnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_meta_features = []\n",
        "test_meta_features = []\n",
        "column_names = []\n",
        "\n",
        "performance_metrics = []\n",
        "\n",
        "# Perform hyperparameter tuning for each model\n",
        "for model_name, model in base_models.items():\n",
        "    print(f\"Tuning hyperparameters for {model_name}...\")\n",
        "\n",
        "    param_grid = param_grids.get(model_name, {})\n",
        "\n",
        "    # If param_grid is not empty, use GridSearchCV\n",
        "    if param_grid:\n",
        "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_model = grid_search.best_estimator_\n",
        "        print(f\"Best hyperparameters for {model_name}: {grid_search.best_params_}\")\n",
        "    else:\n",
        "        # If no hyperparameters to tune (e.g., GaussianNB), fit directly\n",
        "        best_model = model.fit(X_train, y_train)\n",
        "\n",
        "    train_preds = best_model.predict(X_train).reshape(-1, 1)\n",
        "    test_preds = best_model.predict(X_test).reshape(-1, 1)\n",
        "\n",
        "    train_meta_features.append(train_preds)\n",
        "    test_meta_features.append(test_preds)\n",
        "    column_names.append(f'predicted_{model_name}')\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, train_preds)\n",
        "    test_accuracy = accuracy_score(y_test, test_preds)\n",
        "    train_f1 = f1_score(y_train, train_preds, average='weighted')\n",
        "    test_f1 = f1_score(y_test, test_preds, average='weighted')\n",
        "\n",
        "    performance_metrics.append({\n",
        "        'Model': model_name,\n",
        "        'Train Accuracy': train_accuracy,\n",
        "        'Test Accuracy': test_accuracy,\n",
        "        'Train F1 Score': train_f1,\n",
        "        'Test F1 Score': test_f1\n",
        "    })\n",
        "\n",
        "performance_metrics_df = pd.DataFrame(performance_metrics)\n",
        "\n",
        "# Save the performance metrics to an Excel file\n",
        "performance_metrics_df.to_excel('performance_metrics.xlsx', index=False)\n",
        "\n",
        "train_meta_features = np.hstack(train_meta_features)\n",
        "test_meta_features = np.hstack(test_meta_features)\n",
        "\n",
        "train_meta_df = pd.DataFrame(train_meta_features, columns=column_names)\n",
        "test_meta_df = pd.DataFrame(test_meta_features, columns=column_names)\n",
        "\n",
        "train_meta_df['Judgement Status'] = y_train.values\n",
        "test_meta_df['Judgement Status'] = y_test.values\n",
        "\n",
        "combined_meta_df = pd.concat([train_meta_df, test_meta_df])\n",
        "combined_meta_df.to_excel('combined_predictions.xlsx')"
      ],
      "metadata": {
        "id": "-GBc6OAjcXAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Meta-Models"
      ],
      "metadata": {
        "id": "V3bek6orr1Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df = pd.read_excel('/content/combined_predictions.xlsx')\n",
        "\n",
        "X_meta = meta_df.drop(columns='Judgement Status')\n",
        "y_meta = meta_df['Judgement Status']\n",
        "\n",
        "# Split meta_model_data into train and test sets (75:25)\n",
        "X_train_meta, X_test_meta, y_train_meta, y_test_meta = train_test_split(X_meta, y_meta, test_size=0.25, random_state=29)"
      ],
      "metadata": {
        "id": "09DIGJ06joaS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the meta-models with the best hyperparameters (from previous GridSearchCV)\n",
        "meta_models = {\n",
        "    'rf': RandomForestClassifier(\n",
        "        n_estimators=50,\n",
        "        max_depth=10,\n",
        "        min_samples_split=3,\n",
        "        min_samples_leaf=1,\n",
        "        max_features='sqrt'\n",
        "    ),\n",
        "    'svm': SVC(\n",
        "        C=10,\n",
        "        kernel='poly',\n",
        "        degree=2,\n",
        "        gamma='scale',\n",
        "        coef0=1.0\n",
        "    ),\n",
        "    'dt': DecisionTreeClassifier(\n",
        "        max_depth=10,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=3\n",
        "    ),\n",
        "    'xgb': XGBClassifier(\n",
        "        max_depth=7,\n",
        "        learning_rate=0.2,\n",
        "        n_estimators=50,\n",
        "        colsample_bytree=0.8,\n",
        "        subsample=1,\n",
        "        min_child_weight=3\n",
        "    ),\n",
        "    'lgbm': LGBMClassifier(\n",
        "        max_depth=7,\n",
        "        learning_rate=0.1,\n",
        "        n_estimators=200,\n",
        "        colsample_bytree=1.0,\n",
        "        subsample=0.8,\n",
        "        min_child_samples=30\n",
        "    ),\n",
        "    'mlp': MLPClassifier(\n",
        "        hidden_layer_sizes=(50,),\n",
        "        activation='relu',\n",
        "        alpha=0.0001,\n",
        "        max_iter=200\n",
        "    ),\n",
        "    'knn': KNeighborsClassifier(\n",
        "        n_neighbors=9,\n",
        "        weights='uniform',\n",
        "        p=2\n",
        "    ),\n",
        "    'nb': GaussianNB()\n",
        "}\n",
        "\n",
        "train_meta_predictions = []\n",
        "test_meta_predictions = []\n",
        "column_names_meta = []\n",
        "meta_model_metrics = []\n",
        "\n",
        "for model_name, meta_model in meta_models.items():\n",
        "\n",
        "    meta_model.fit(X_train_meta, y_train_meta)\n",
        "\n",
        "    train_meta_preds = meta_model.predict(X_train_meta).reshape(-1, 1)\n",
        "    test_meta_preds = meta_model.predict(X_test_meta).reshape(-1, 1)\n",
        "\n",
        "    train_meta_predictions.append(train_meta_preds)\n",
        "    test_meta_predictions.append(test_meta_preds)\n",
        "    column_names_meta.append(f'meta_predicted_{model_name}')\n",
        "\n",
        "    train_accuracy_meta = accuracy_score(y_train_meta, train_meta_preds)\n",
        "    test_accuracy_meta = accuracy_score(y_test_meta, test_meta_preds)\n",
        "    train_f1_meta = f1_score(y_train_meta, train_meta_preds, average='weighted')\n",
        "    test_f1_meta = f1_score(y_test_meta, test_meta_preds, average='weighted')\n",
        "\n",
        "    meta_model_metrics.append({\n",
        "        'Model': model_name,\n",
        "        'Train Accuracy': train_accuracy_meta,\n",
        "        'Test Accuracy': test_accuracy_meta,\n",
        "        'Train F1 Score': train_f1_meta,\n",
        "        'Test F1 Score': test_f1_meta\n",
        "    })\n",
        "\n",
        "meta_model_metrics_df = pd.DataFrame(meta_model_metrics)\n",
        "\n",
        "# Save the meta-model metrics to an Excel file\n",
        "meta_model_metrics_df.to_excel('meta_model_metrics.xlsx', index=False)\n",
        "\n",
        "train_meta_predictions = np.hstack(train_meta_predictions)\n",
        "test_meta_predictions = np.hstack(test_meta_predictions)\n",
        "\n",
        "train_meta_pred_df = pd.DataFrame(train_meta_predictions, columns=column_names)\n",
        "test_meta_pred_df = pd.DataFrame(test_meta_predictions, columns=column_names)\n",
        "\n",
        "train_meta_pred_df['Judgement Status'] = y_train.values\n",
        "test_meta_pred_df['Judgement Status'] = y_test.values\n",
        "\n",
        "combined_meta_pred_df = pd.concat([train_meta_pred_df, test_meta_pred_df])\n",
        "\n",
        "# Saving all meta-model predictions to a single Excel file\n",
        "combined_meta_pred_df.to_excel('final_predictions.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "KSNHYQyhcxqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explainability with LIME"
      ],
      "metadata": {
        "id": "xDQ-biFwsKhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba_base_models(texts):\n",
        "    print(\"Input texts to predict_proba_base_models:\", texts)\n",
        "\n",
        "    # Generate embeddings for a test input text\n",
        "    embeddings = np.array([inlegalbert_embed(text, inlegalbert_model, tokenizer) for text in texts])\n",
        "    print(\"Generated embeddings shape:\", embeddings.shape)\n",
        "\n",
        "    embeddings_df = pd.DataFrame(embeddings)\n",
        "    embeddings_df.to_excel('current_embeddings_lime_generated.xlsx')\n",
        "\n",
        "    # Generate base model predictions\n",
        "    base_model_preds = [model.predict(embeddings) for _, model in base_models.values()]\n",
        "\n",
        "    for i, preds in enumerate(base_model_preds):\n",
        "        print(f\"Base model {i} predictions shape:\", preds.shape)\n",
        "\n",
        "    # Stack base model predictions\n",
        "    base_model_preds_stacked = np.hstack([preds.reshape(-1, 1) for preds in base_model_preds])\n",
        "    print(\"Stacked base model predictions shape:\", base_model_preds_stacked.shape)\n",
        "\n",
        "    # Save base model predictions to a DataFrame\n",
        "    base_model_preds_df = pd.DataFrame(base_model_preds_stacked, columns=[f'predicted_{name}' for name, _ in base_models.items()])\n",
        "    base_model_preds_df.to_excel(\"current_base_model_predictions_lime_generated.xlsx\")\n",
        "\n",
        "    return meta_model.predict_proba(base_model_preds_stacked)\n"
      ],
      "metadata": {
        "id": "lclVpijmniFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_meta = pd.concat([X_test_meta, y_test_meta], axis=1)\n",
        "idx = test_df_meta.index[120] #Lime predictions for the record with index 120 in the test dataframe of meta-model\n",
        "text_example = df[\"Processed_Extractive_Summaries\"].iloc[idx]\n",
        "print(text_example)\n",
        "\n",
        "# Initialize LIME explainer\n",
        "class_names = [0, 1, 2, 3]\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "# Generate LIME explanation\n",
        "exp = explainer.explain_instance(\n",
        "    text_example,\n",
        "    lambda x: predict_proba_base_models(x),\n",
        "    num_features=15,\n",
        "    top_labels=len(class_names)\n",
        ")\n",
        "\n",
        "# Print and visualize the explanation for each class\n",
        "for label in class_names:\n",
        "    print(f\"\\nExplanation for class {label}:\")\n",
        "    explanation_text = exp.as_list(label=label)\n",
        "    print(explanation_text)\n",
        "\n",
        "    exp.show_in_notebook(text=text_example, labels=[label])\n",
        "\n",
        "    # Plot the LIME explanation\n",
        "    weights = OrderedDict(exp.as_list(label=label))\n",
        "    lime_weights = pd.DataFrame({\"words\": list(weights.keys()), \"weights\": list(weights.values())})\n",
        "\n",
        "    # Define custom colors\n",
        "    custom_colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\"]\n",
        "\n",
        "    # Create the bar plot with custom colors\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=\"words\", y=\"weights\", data=lime_weights, palette=custom_colors[:len(lime_weights)])\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title(f\"Class {label} - Features Weights Given by LIME\")\n",
        "    plt.show()\n",
        "\n",
        "# Save the LIME explanation object\n",
        "with open('120_lime_explanation_of_all_classes.pkl', 'wb') as file:\n",
        "    pickle.dump(exp, file)"
      ],
      "metadata": {
        "id": "HZ-lFw3HqsO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}